# NLP-from-scratch

SENTENCE CLASSIFICATION FROM SCRATCH

NLP practitioners often have to solve a problem from scratch, which includes gathering and cleaning data, annotating the data, choosing a model, iterating on the model, and possibly going back to change the data.

The full process will require the following steps:
1. Understand the task specification
2. Collect raw data
3. Annotate test and training data for development
4. Train and test models using this data
5. ”Deploy” your System

Data Source Description:
To generate unlabeled data, I employed chatgpt by requesting sentences related to same-sex marriage and immigration topics covering various issues which are used as label names in the data. I decided to use chatgpt as it would give me more precise sentences related to a particular issueon the specific domain in different languages. Chatgpt was provided an issue, say economic, and I asked for sentences relating to immigration or same-sex marriage. Using these sentences as a reference, I created additional sentences on my own. To label the data, I carefully read each sentence and assigned the labels I believed were the most suitable based on the categories mentioned in the report. I ensured that the labels were consistent and relevant to the sentence’s content, as the annotated data is being used for further analysis and training of the models. For non-English sentences, I used Google Translate to convert them to English before applying labels. In general, I chose to annotate more data than the minimum amount of data because I believe that large datasets improve can machine learning accuracy and robustness.

Data Agregation Startegy:
To produce the final dataset, it was necessary to aggregate the annotations from both annotators. In cases where the annotators disagreed on the label assigned to a particular sentence, a consensus needed to be reached. Random selection of a label was not an option, as this could lead to unreliable results. To handle disagreements, I approached the annotators to discuss each disagreement and come to a consensus. This allowed all of us to work together on the dataset and to find a mutually agreeable labels to sentences. In some cases, I needed to make the final decision on the label. To do this, I considered both annotators’ opinions and chose the label that I believed was most appropriate for the sentence. I created a new file named ”aggregated data.tsv.” This file has the same format as the original “first data.tsv” and contains the final, agreed-upon labels for each sentence. This allowed me for a consistent dataset that is supposed to be used for analysis in the further part of the assignment.

Model Training and testing:
I chose the ”nlptown/bert-base-multilingual-uncased-sentiment” model instead of the ”base-bert uncased” model in order to improve performance since I was only getting 18 correct predictions with the given model. I used the first data.tsv, annotated data, seed.tsv as well as the augmented data combined to train the model. My experiments included altering the epoch number, learning rate, dropout rate, and fine tuning the layers. In order to tune hyperparameters, I used a trial-and error method of trying different values and then evaluating the model’s performance according to the results. I tried different batch sizes, such as 16, 32, and 64. I used the dropout function to prevent overfitting during training. After experimenting with different values for the number of epochs, I observed that there was no significant change after a certain number of epochs. So I set the numner of epochs to 10. Different learning rates were tried like 3e-4, 4e-4 and I found that 5e-5 gave the best result for my model. Furthermore, I adjusted the batch size and fine-tuned the layers in addition to the hyperparameters. The best results were obtained when I set the Fine tine layers to 10. After changing the model and adjusting all the hyperparameters,the number of correct predictaions was 102.The accuracy of the test set is 56% and that of the validation set is 64%There was an increase in accuracy and number of correct predictions for the validation set.

Error Analysis:
An error analysis is performed on a model in order to gain insight into what types of errorsit makes and where improvements can be made. We can improve the model’s performance by identifying patterns or common linguistic properties the model struggles with by analyzing the examples that the model gets wrong. The code for error analysis was written using the get validation function as reference and printed out 5 incorrect predictions. A lack of diversity in data may be a contributing factor to errors as there are few sentences in different languages in my dataset. Paraphrases are used in most sentences because of data augmentation. There are also some sentences translated from English to another language and vice-versa. In addition, it is possible that the sentences are labeled incorrectly, resulting in wrong predictions being made because of the incorrect data used to train the model. Checking the labels of the training data can improve the model’s performance. Training the model on more data by increasing, the size of the dataset. Furthermore, we can train the model on data that includes more sentences in different languages. It is also possible to improve results by changing the hyperparameters.

Data Augmentation:
To augment the data, I used a combination of paraphrasing and translation techniques. For paraphrasing, I took the sentences from the first data.tsv and rephrased them while keeping the original label ID and label names intact. This helped to increase the diversity of the training data and reduced the chances of overfitting. I also used the ’google/pegasus-xsum’ model to augment my data. I generated over 800 sentences using this model and added it to my dataset. Additionally, since my dataset contained sentences in multiple languages, I used translation to convert non-English sentences to English. This helped to increase the size of the dataset and improve the model’s performance on non-English text. After performing data augmentation, I trained the model on the augmented dataset and evaluated its performance on the test set. The results showed that data augmentation resulted in improved performance on the test set compared to the model trained on the final data that consisted of first data and annotator data alone. I believe this happened because data augmentation helped to increase the diversity and size of the training dataset, which in turn improved the model’s ability to generalize to new examples. By providing more varied examples, the model was able to learn more robust features of the training data.

Model Deployment:
The final ”deployment” of your model will consist of running your (best) model over a private test set.
